function [trainedModel, validationRMSE, validationPredictions, results] = trainRegressionModel(trainingData, responseName, features, model_name)
% Regression model trainer
% Inputs:
% 
% trainingData: a table containing both input features and the target
% feature.
% responseName: name of the target feature in the trainingData table.
% features: name of the input features.
% model_name: one of the following: % EnsembleBag = Bagged Ensemble of trees
%                                     EnsembleBoost = Boosted Ensemble of trees
%                                     NN1 = A neural network model with 1 hidden layer
%                                     NN2 = A neural network model with 2 hidden layer
%                                     NN3 = A neural network model with 3 hidden layer    
%                                     SVMPly = SVM with polynomial kernel
%                                     SVMGaussian = SVM with Gaussian kernel
%                                     LR = Linear Regression
% ---------------------------------------------------------------------------------------------------
% Outputs:
% trainedModel: a fully trained regression model
% validationRMSE: cross validation RMSE
% validationPredictions: predictions for cross validation
% results: hyperparameter optimization results
% 
% How to predict with a trained model:
% 'To make predictions on a new table, T, use: 
%        yfit = c.predictFcn(T) 
%      replacing 'c' with the name of the variable that is this struct, e.g. 'trainedModel'. 
% 
%      The table, T, must contain the variables returned by: 
%        c.RequiredVariables 
%      Variable formats (e.g. matrix/vector, datatype) must match the original training data. 
%      Additional variables are ignored. 


inputTable = trainingData;
predictorNames = features;
predictors = inputTable(:, predictorNames);
response = inputTable{:, responseName};

switch model_name
case 'EnsembleBag'
        nLearning = optimizableVariable('NumLearningCycles',[1,100],'Type','integer');
        MaxSplit = optimizableVariable('MaxNumSplits',[1,size(predictors,1)],'Type','integer');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrensemble(predictors, ...
                                        response, ...
                                        'Method', 'Bag', ...
                                        'CVPartition',c,...
                                        'NumLearningCycles', x.NumLearningCycles, ...,
                                        'Learners', templateTree('MaxNumSplits', x.MaxNumSplits,'NumVariablesToSample', 'all')), ...
                                        "LossFun",'mse');
        results = bayesopt(fun,[nLearning,MaxSplit],'Verbose',0,...
                            'AcquisitionFunctionName','expected-improvement-plus', ...
                                 'PlotFcn',[],...
                                 'MaxObjectiveEvaluations',30);
        model = fitrensemble(predictors, ...
                                response, ...
                                'Method', 'Bag', ...
                                'NumLearningCycles', results.bestPoint.NumLearningCycles, ...,
                                'Learners', templateTree('MaxNumSplits', results.bestPoint.MaxNumSplits,'NumVariablesToSample', 'all'));
    case 'EnsembleBoost'
        nLearning = optimizableVariable('NumLearningCycles',[1,200],'Type','integer');
        MaxSplit = optimizableVariable('MaxNumSplits',[1,size(predictors,1)],'Type','integer');
        Lrate = optimizableVariable('Lrate',[0.01,1.0],'Type','real');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrensemble(predictors, ...
                                    response, ...
                                    'Method', 'LSBoost', ...
                                    'CVPartition',c,...
                                    'NumLearningCycles', x.NumLearningCycles, ...,
                                    'LearnRate', x.Lrate,...
                                    'Learners', templateTree('MaxNumSplits', x.MaxNumSplits,'NumVariablesToSample', 'all')),"LossFun",'mse');
        results = bayesopt(fun,[nLearning,MaxSplit,Lrate],'Verbose',0,...
            'AcquisitionFunctionName','expected-improvement-plus', ...
            'PlotFcn',[],...
            'MaxObjectiveEvaluations',30);
        
        model = fitrensemble(predictors, ...
                             response, ...
                            'Method', 'LSBoost', ...
                            'NumLearningCycles', results.bestPoint.NumLearningCycles, ...
                            'Learners', templateTree('MaxNumSplits', results.bestPoint.MaxNumSplits,'NumVariablesToSample', 'all'), ...
                            'LearnRate', results.bestPoint.Lrate);
         case 'NN3'
        Layer1 = optimizableVariable('Layer1',[10,100],'Type','integer');
        Layer2 = optimizableVariable('Layer2',[10,100],'Type','integer');
        Layer3 = optimizableVariable('Layer3',[10,100],'Type','integer');
        lambda = optimizableVariable('Lambda',[0,1],'Type','real');
        act = optimizableVariable('Activations', {'tanh', 'sigmoid', 'relu'}, 'Type','categorical');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrnet(predictors, ...
                                    response, ...
                                    'CVPartition',c,...
                                    'LayerSizes', [x.Layer1, x.Layer2, x.Layer3], ...
                                    'Activations', char(x.Activations), ...
                                    'Lambda', x.Lambda, ...
                                    'IterationLimit', 300, ...
                                    'Standardize', true),"LossFun",'mse');
        results = bayesopt(fun,[Layer1, Layer2, Layer3,act, lambda],'Verbose',0,...
            'AcquisitionFunctionName','expected-improvement-plus', ...
            'PlotFcn',[],...
            'MaxObjectiveEvaluations',30);
        l1 = results.bestPoint.Layer1; l2 = results.bestPoint.Layer2; l3 = results.bestPoint.Layer3;

        model = fitrnet(predictors, ...
                        response, ...
                        'LayerSizes', [l1 l2 l3], ...
                        'Activations', char(results.bestPoint.Activations), ...
                        'Lambda', results.bestPoint.Lambda, ...
                        'IterationLimit', 300, ...
                        'Standardize', true);
    case 'NN2'
        Layer1 = optimizableVariable('Layer1',[10,150],'Type','integer');
        Layer2 = optimizableVariable('Layer2',[10,150],'Type','integer');
        act = optimizableVariable('Activations', {'tanh', 'sigmoid', 'relu'}, 'Type','categorical');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrnet(predictors, ...
                                    response, ...
                                    'CVPartition',c,...
                                    'LayerSizes', [x.Layer1, x.Layer2], ...
                                    'Activations', char(x.Activations), ...
                                    'Lambda', 0, ...
                                    'IterationLimit', 300, ...
                                    'Standardize', true),"LossFun",'mse');
        results = bayesopt(fun,[Layer1, Layer2,act],'Verbose',0,...
            'AcquisitionFunctionName','expected-improvement-plus', ...
            'PlotFcn',[],...
            'MaxObjectiveEvaluations',30);
        l1 = results.bestPoint.Layer1; l2 = results.bestPoint.Layer2;

        model = fitrnet(predictors, ...
                        response, ...
                        'LayerSizes', [l1 l2], ...
                        'Activations', char(results.bestPoint.Activations), ...
                        'Lambda', 0, ...
                        'IterationLimit', 300, ...
                        'Standardize', true);
    case 'NN1'
        Layer1 = optimizableVariable('Layer1',[10,300],'Type','integer');
        act = optimizableVariable('Activations', {'tanh', 'sigmoid', 'relu'}, 'Type','categorical');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrnet(predictors, ...
                                    response, ...
                                    "CVPartition",c,...
                                    'LayerSizes', x.Layer1, ...
                                    'Activations', char(x.Activations), ...
                                    'Lambda', 0, ...
                                    'IterationLimit', 300, ...
                                    'Standardize', true),"LossFun",'mse');
        results = bayesopt(fun,[Layer1, act],'Verbose',0,...
            'AcquisitionFunctionName','expected-improvement-plus', ...
            'PlotFcn',[],...
            'MaxObjectiveEvaluations',30);
        l1 = results.bestPoint.Layer1;

        model = fitrnet(predictors, ...
                        response, ...
                        'LayerSizes', l1, ...
                        'Activations', char(results.bestPoint.Activations), ...
                        'Lambda', 0, ...
                        'IterationLimit', 300, ...
                        'Standardize', true);
    
    case 'SVMPly'
        order = optimizableVariable('order',[2,5],'Type','integer');
        scale = optimizableVariable('scale',[0.1,10],'Type','real');
        box = optimizableVariable('box',[1,50],'Type','real');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrsvm(predictors, ...
                                    response, ...
                                    "CVPartition",c,...
                                    'KernelFunction', 'polynomial', ...
                                    'PolynomialOrder', x.order, ...
                                    'KernelScale', x.scale, ...
                                    'BoxConstraint', x.box, ...
                                    'Standardize', true),"LossFun",'mse');
        results = bayesopt(fun,[order, scale, box],'Verbose',0,...
            'AcquisitionFunctionName','expected-improvement-plus', ...
            'PlotFcn',[],...
            'MaxObjectiveEvaluations',30);

    model = fitrsvm(predictors, ...
                    response, ...
                    'KernelFunction', 'polynomial', ...
                    'PolynomialOrder', results.bestPoint.order, ...
                    'KernelScale', results.bestPoint.scale, ...
                    'BoxConstraint', results.bestPoint.box, ...
                    'Standardize', true);
    case 'SVMGaussian'
        scale = optimizableVariable('scale',[0.1,30],'Type','real');
        box = optimizableVariable('box',[1,50],'Type','real');
        c = cvpartition(size(predictors,1),'Kfold',10);
        fun = @(x)kfoldLoss(fitrsvm(predictors, ...
                                    response, ...
                                    "CVPartition",c,...
                                    'KernelFunction', 'gaussian', ...
                                    'KernelScale', x.scale, ...
                                    'BoxConstraint', x.box, ...
                                    'Standardize', true),"LossFun",'mse');
        results = bayesopt(fun,[scale, box],'Verbose',0,...
            'AcquisitionFunctionName','expected-improvement-plus', ...
            'PlotFcn',[],...
            'MaxObjectiveEvaluations',30);

    model = fitrsvm(predictors, ...
                    response, ...
                    'KernelFunction', 'gaussian', ...
                    'KernelScale', results.bestPoint.scale, ...
                    'BoxConstraint', results.bestPoint.box, ...
                    'Standardize', true);
    case 'LR'
        ConcData = predictors;
        ConcData.gdpRatio = response;
        model = fitlm(ConcData, ...
                        'interactions', ...
                        'RobustOpts', 'off');
        results = [];
end

% Create the result struct with predict function
predictorExtractionFcn = @(t) t(:, predictorNames);
ensemblePredictFcn = @(x) predict(model, x);
trainedModel.predictFcn = @(x) ensemblePredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedModel.RequiredVariables = features;
trainedModel.Model = model;
KFolds = 10;
% Perform cross-validation
if ~strcmp(model_name, 'LR')
    partitionedModel = crossval(trainedModel.Model, 'KFold', KFolds);
    
    % Compute validation predictions
    validationPredictions = kfoldPredict(partitionedModel);
    
    % Compute validation RMSE
    validationRMSE = sqrt(kfoldLoss(partitionedModel, 'LossFun', 'mse'));
else
    cvp = cvpartition(size(response, 1), 'KFold', KFolds);
    % Initialize the predictions to the proper sizes
    validationPredictions = response;
    for fold = 1:KFolds
        trainingPredictors = predictors(cvp.training(fold), :);
        trainingResponse = response(cvp.training(fold), :);
    
        % Train a regression model
        % This code specifies all the model options and trains the model.
        Cpred = trainingPredictors;
        Cpred.gdpRatio = trainingResponse;
        linearModel = fitlm(...
            Cpred, ...
            'interactions', ...
            'RobustOpts', 'off');
    
        % Create the result struct with predict function
        linearModelPredictFcn = @(x) predict(linearModel, x);
        validationPredictFcn = @(x) linearModelPredictFcn(x);
    
        % Compute validation predictions
        validationPredictors = predictors(cvp.test(fold), :);
        foldPredictions = validationPredictFcn(validationPredictors);
    
        % Store predictions in the original order
        validationPredictions(cvp.test(fold), :) = foldPredictions;
    end
    
        % Compute validation RMSE
        isNotMissing = ~isnan(validationPredictions) & ~isnan(response);
        validationRMSE = sqrt(sum((validationPredictions - response ).^2) / numel(response(isNotMissing) ));
end

end